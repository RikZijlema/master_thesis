{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"scraping.ipynb","provenance":[],"authorship_tag":"ABX9TyPgxZORYDKuzTMiWhnQU0GT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"It_TeOOsmWis","executionInfo":{"status":"ok","timestamp":1617178023388,"user_tz":-120,"elapsed":1034,"user":{"displayName":"Rik Zijlema","photoUrl":"","userId":"16715734862350772008"}}},"source":["import requests\n","import re\n","import pandas as pd\n","from bs4 import BeautifulSoup as bs"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7SamlDFFNKA"},"source":["niburu_dataset = pd.DataFrame(columns=[\"url\",\"title\",\"article\",\"date\"])\n","for i in range(11500,16135):\n","    url = F\"https://niburu.co/{i}\"\n","    page = requests.get(url)\n","    soup = bs(page.content, \"html.parser\")\n","    #If there are embedded tweets, delete them\n","    if soup.find(\"blockquote\", {\"class\": \"twitter-tweet\"}):\n","        blockquote = soup.find(\"blockquote\", {\"class\": \"twitter-tweet\"})\n","        blockquote.decompose()\n","    if soup.find(\"div\", itemprop=\"articleBody\"):\n","        article = soup.find(\"div\", itemprop=\"articleBody\").getText()\n","    else:\n","        article = \"None\"\n","    if soup.find(\"h1\"):\n","        title = soup.find(\"h1\").getText()\n","    else:\n","        title = \"None\"\n","    if soup.find(\"time\"):\n","        date = soup.find(\"time\").getText()\n","    data = {\"url\": url, \"title\": title, \"article\": article, \"date\": date}\n","    niburu_dataset = niburu_dataset.append(data, ignore_index=True)\n","    print(i)\n","niburu_dataset.to_csv(\"niburu.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7pIhEpa0vLNi"},"source":["ninefornews_dataset = pd.DataFrame(columns=[\"url\",\"title\",\"article\"])\n","for i in range(1,501):\n","    url = F\"https://www.ninefornews.nl/page/{i}/\"\n","    page = requests.get(url)\n","    soup = bs(page.content,\"html.parser\")\n","    links = soup.findAll(\"a\", {\"class\": \"more-link\"}, href=True)\n","    for l in links:\n","        page = requests.get(l[\"href\"])\n","        soup = bs(page.content, \"html.parser\")\n","        if soup.find(\"h1\"):\n","            title = soup.find(\"h1\").text\n","        else:\n","            title = \"None\"\n","        if soup.find(\"span\", {\"class\": \"tie-date\"}):\n","            time = soup.find(\"span\", {\"class\": \"tie-date\"}).text\n","        else:\n","            time = \"none\"\n","        if soup.find(\"div\", {\"class\": \"entry\"}):\n","            div = soup.find(\"div\", {\"class\": \"entry\"})\n","            article = div.findChildren(\"p\", recursive = False)\n","            full_article = \"\"\n","            for paragraph in article:\n","                full_article += paragraph.text + \" \"\n","        else:\n","            full_article = \"None\"\n","        data = {\"url\": l[\"href\"], \"title\": title, \"article\": full_article, \"time\": time}\n","        ninefornews_dataset = ninefornews_dataset.append(data, ignore_index=True)\n","    print(i)\n","ninefornews_dataset.to_csv(\"ninefornews.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0F0neNacoY7"},"source":["nu_dataset = pd.DataFrame(columns=[\"url\",\"title\",\"article\", \"date\"])\n","for i in range(6120487,6123500):\n","    url = F\"https://nu.nl/artikel/{i}\"\n","    page = requests.get(url)\n","    soup = bs(page.content, \"html.parser\")\n","    #If there are embedded tweets, delete them\n","    if soup.select(\".tweet-content p\"):\n","        tweet_list = soup.select(\".tweet-content p\")\n","        for tweet in tweet_list:\n","            tweet.decompose()\n","    if soup.select(\".article-body p\"):\n","        article_p = soup.select(\".article-body p\")\n","        full_article = \"\"\n","        for p in article_p:\n","            full_article += p.text + \" \"\n","    else:\n","        full_article = \"None\"\n","    if soup.find(\"h1\"):\n","        title = soup.find(\"h1\").text\n","    else:\n","        title = \"None\"\n","    if soup.find(\"span\", {\"class\": \"pubdate\"}):\n","        date = soup.find(\"span\", {\"class\": \"pubdate\"}).text\n","    else:\n","        date = \"None\"\n","    data = {\"url\": url, \"title\": title, \"article\": full_article, \"date\": date}\n","    nu_dataset = nu_dataset.append(data, ignore_index=True)\n","    print(i)\n","nu_dataset.to_csv(\"nu.csv\",index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8zv7T5zf7Ak"},"source":["mv_dataset = pd.DataFrame(columns=[\"url\",\"title\",\"article\",\"time\"])\n","for i in range(1,133):\n","    url = F\"https://martinvrijland.nl/page/{i}/\"\n","    page = requests.get(url)\n","    soup = bs(page.content, \"html.parser\")\n","    links = soup.find_all(\"a\", {\"class\": \"readmore\"}, href=True)\n","    for l in links:\n","        page = requests.get(l[\"href\"])\n","        soup = bs(page.content, \"html.parser\")\n","        if soup.find(\"h1\"):\n","            title = soup.find(\"h1\").text\n","        else:\n","            title = \"None\"\n","        if soup.select(\".post_header > p\"):\n","            body = soup.select(\".post_header > p\")\n","            full_article = \"\"\n","            for item in body:\n","                if item.text[0:21] == \"Bron linkvermeldingen\":\n","                    pass\n","                else:\n","                    full_article += item.text + \" \"\n","        else:\n","            full_article = \"None\"  \n","        if soup.find(\"span\"):\n","           time = soup.find(\"span\").text\n","        else:\n","           time = \"None\"\n","        data = {\"url\": l[\"href\"], \"title\": title, \"article\": full_article, \"time\": time}\n","        mv_dataset = mv_dataset.append(data, ignore_index=True)\n","    print(i)\n","mv_dataset.to_csv(\"mv.csv\",index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UuphGZTKEYw4"},"source":["speld_wet_dataset = pd.DataFrame(columns=[\"url\",\"title\",\"article\",\"time\"])\n","for i in range(1,42):\n","    url = F\"https://speld.nl/category/wetenschap/page/{i}/\"\n","    page = requests.get(url)\n","    soup = bs(page.content, \"html.parser\")\n","    links = soup.select(\".articles-content a.divlink\")\n","    for l in links:\n","        page = requests.get(l[\"href\"])\n","        soup = bs(page.content, \"html.parser\")\n","        if soup.find(\"h1\"):\n","            title = soup.find(\"h1\").text\n","        else:\n","            title = \"None\"\n","        if soup.select(\"[itemprop='articleBody'] p\"):\n","            body = soup.select(\"[itemprop='articleBody'] p\")\n","            full_article = \"\"\n","            for item in body:\n","                full_article += item.text + \" \"\n","        else:\n","            full_article = \"None\" \n","        if soup.find(\"p\", class_=\"byline\"):\n","           time = soup.find(\"p\", class_=\"byline\").text\n","        else:\n","           time = \"None\"\n","        data = {\"url\": l[\"href\"], \"title\": title, \"article\": full_article, \"time\": time}\n","        speld_wet_dataset = speld_wet_dataset.append(data, ignore_index=True)\n","    print(i)\n","speld_wet_dataset.to_csv(\"speld_wet.csv\",index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-IDUbU5yplMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617185596955,"user_tz":-120,"elapsed":217697,"user":{"displayName":"Rik Zijlema","photoUrl":"","userId":"16715734862350772008"}},"outputId":"4848b5c8-b2ac-4e7a-8a13-c50872e04259"},"source":["nieuwspaal_bui_dataset = pd.DataFrame(columns=[\"url\",\"title\",\"article\",\"date\"])\n","for i in range(1,15):\n","    url = F\"https://nieuwspaal.nl/category/sport/page/{i}/\"\n","    page = requests.get(url)\n","    soup = bs(page.content, \"html.parser\")\n","    links = soup.select(\".entry > a\")\n","    for l in links:\n","        page = requests.get(l[\"href\"])\n","        soup = bs(page.content, \"html.parser\")\n","        if soup.select_one(\"p.wp-caption-text\"):\n","            soup.select_one(\"p.wp-caption-text\").decompose()\n","        if soup.find(\"h2\"):\n","            title = soup.find(\"h2\").text\n","        else:\n","            title = None\n","        if soup.select(\"div.articlebody p\"):\n","            article = soup.select(\"div.articlebody p\")\n","            full_article = \"\"\n","            for item in article:\n","                full_article += item.text + \" \"\n","        else:\n","            full_article = None\n","        if soup.find(\"time\"):\n","            date = soup.find(\"time\").text\n","        else:\n","            date = None\n","        data = {\"url\": l[\"href\"], \"title\": title, \"article\": full_article, \"date\": date}\n","        nieuwspaal_bui_dataset = nieuwspaal_bui_dataset.append(data, ignore_index=True)\n","    print(i)\n","nieuwspaal_bui_dataset.to_csv(\"nieuwspaal_sport.csv\",index=False)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CIaX75st_0t2"},"source":["dlm_dataset = pd.DataFrame(columns=[\"url\",\"title\",\"article\",\"date\"])\n","for i in range(1,90):\n","    url = F\"https://www.dlmplus.nl/blog/page/{i}/\"\n","    page = requests.get(url)\n","    soup = bs(page.content, \"html.parser\")\n","    links = soup.select(\".entry-title a\")\n","    for l in links:\n","        page = requests.get(l[\"href\"])\n","        soup = bs(page.content, \"html.parser\")\n","        if soup.find(\"h1\"):\n","            title = soup.find(\"h1\").text\n","        else:\n","            title = None\n","        if soup.select(\"div.entry-content p\"):\n","            body = soup.select(\"div.entry-content p\")\n","            full_article = \"\"\n","            for item in body:\n","                full_article += item.text + \" \"\n","        else:\n","            full_article = None\n","        if soup.select_one(\".entry-meta-date a\"):\n","            date = soup.select_one(\".entry-meta-date a\").text\n","        else:\n","            date = None\n","        data = {\"url\": l[\"href\"], \"title\": title, \"article\": full_article, \"date\": date}\n","        dlm_dataset = dlm_dataset.append(data, ignore_index=True)\n","    print(i)\n","dlm_dataset.to_csv(\"dlm.csv\",index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0tujWYIK-v-J"},"source":["tel_dataset = pd.DataFrame(columns=[\"url\",\"title\",\"article\",\"date\"])\n","for i in range(1,28):\n","    url = F\"https://www.telegraaf.nl/archief/2021/3/{i}\"\n","    page = requests.get(url)\n","    soup = bs(page.content, \"html.parser\")\n","    links = soup.select(\"a.ArchivePage__link\")\n","    for l in links:\n","        url = \"https://www.telegraaf.nl\" + l[\"href\"]\n","        page = requests.get(url)\n","        soup = bs(page.content, \"html.parser\")\n","        if soup.find(\"h1\"):\n","            title = soup.find(\"h1\").text\n","        else:\n","            title = None\n","        if soup.select(\"p.ArticleIntro__paragraph, p.ArticleBodyBlocks__paragraph\"):\n","            body = soup.select(\"p.ArticleIntro__paragraph, p.ArticleBodyBlocks__paragraph\")\n","            full_article = \"\"\n","            for item in body:\n","                full_article += item.text + \" \"\n","        else:\n","            full_article = None\n","        if soup.select_one(\".ArticleAuthorBlock__timestamp time\"):\n","            date = soup.select_one(\".ArticleAuthorBlock__timestamp time\").text\n","        else:\n","            date = None\n","        data = {\"url\": url, \"title\": title, \"article\": full_article, \"date\": date}\n","        tel_dataset = tel_dataset.append(data, ignore_index=True)\n","    print(i)\n","tel_dataset.to_csv(\"tel.csv\",index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-V3q9V7IUrXs"},"source":["transweb_dataset = pd.DataFrame(columns=[\"url\",\"title\",\"article\",\"date\"])\n","for i in range(1,25):\n","    url = F\"https://www.transitieweb.nl/page/{i}/\"\n","    page = requests.get(url)\n","    soup = bs(page.content, \"html.parser\")\n","    links = soup.select(\".post-title a\")\n","    for l in links:\n","        page = requests.get(l[\"href\"])\n","        soup = bs(page.content, \"html.parser\")\n","        if soup.find(\"h1\"):\n","            title = soup.find(\"h1\").text\n","        else:\n","            title = None\n","        if soup.select(\".entry-inner > p\"):\n","            body = soup.select(\".entry-inner > p\")\n","            full_article = \"\"\n","            for item in body:\n","                full_article += item.text + \" \"\n","        else:\n","            full_article = None\n","        if soup.select_one(\"span.published\"):\n","            date = soup.select_one(\"span.published\").text\n","        else:\n","            date = None\n","        data = {\"url\": l[\"href\"], \"title\": title, \"article\": full_article, \"date\": date}\n","        transweb_dataset = transweb_dataset.append(data, ignore_index=True)\n","    print(i)\n","transweb_dataset.to_csv(\"transweb.csv\",index=False)"],"execution_count":null,"outputs":[]}]}